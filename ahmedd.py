# import streamlit as st
# import numpy as np
# import cv2
# from PIL import Image
# import tensorflow as tf
# from tensorflow.keras.applications import ResNet50, InceptionV3, VGG16
# from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess
# from tensorflow.keras.applications.inception_v3 import preprocess_input as inception_preprocess
# from tensorflow.keras.applications.vgg16 import preprocess_input as vgg_preprocess
# from tensorflow.keras.models import Model
# from tensorflow.keras.layers import GlobalAveragePooling2D, Dense

# # ----------------------------- Config -----------------------------
# st.set_page_config(page_title="ØªØµÙ†ÙŠÙ Ø§Ù„ÙˆØ¬ÙˆÙ‡", layout="centered")

# # Ø®Ù„ÙÙŠØ© Ø£Ù†ÙŠÙ‚Ø©
# page_bg = '''
# <style>
# .stApp { background: linear-gradient(135deg, #e3f2fd, #f8bbd0); }
# h1, h2, h3 { color: #01579b; }
# </style>
# '''
# st.markdown(page_bg, unsafe_allow_html=True)

# # ----------------------------- Ø¹Ù†ÙˆØ§Ù† -----------------------------
# st.markdown("<h1 style='text-align: center; color: #1976d2;'>ğŸ§‘ ØªØµÙ†ÙŠÙ Ø§Ù„ÙˆØ¬ÙˆÙ‡</h1>", unsafe_allow_html=True)
# st.markdown("<h2 style='text-align: center; color: #d81b60;'>Face Classification Project</h2>", unsafe_allow_html=True)

# st.markdown("---")

# # ----------------------------- Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ -----------------------------
# fake_names = ["Ø£Ø­Ù…Ø¯", "Ø³Ø§Ø±Ø©", "Ù…Ø­Ù…Ø¯", "ÙØ§Ø·Ù…Ø©", "Ø¹Ù„ÙŠ", "Ù†ÙˆØ±", "Ø¹Ù…Ø±", "Ù„ÙŠÙ„Ù‰"]
# num_classes = len(fake_names)

# # ----------------------------- ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù€ 3 Ù…ÙˆØ¯ÙŠÙ„Ø§Øª -----------------------------
# @st.cache_resource
# def load_models():
#     # ResNet50
#     base_resnet = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
#     base_resnet.trainable = False
#     x = GlobalAveragePooling2D()(base_resnet.output)
#     x = Dense(512, activation='relu')(x)
#     pred_resnet = Dense(num_classes, activation='softmax')(x)
#     model_resnet = Model(base_resnet.input, pred_resnet)

#     # InceptionV3
#     base_inception = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))
#     base_inception.trainable = False
#     x = GlobalAveragePooling2D()(base_inception.output)
#     x = Dense(512, activation='relu')(x)
#     pred_inception = Dense(num_classes, activation='softmax')(x)
#     model_inception = Model(base_inception.input, pred_inception)

#     # VGG16
#     base_vgg = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
#     base_vgg.trainable = False
#     x = GlobalAveragePooling2D()(base_vgg.output)
#     x = Dense(512, activation='relu')(x)
#     pred_vgg = Dense(num_classes, activation='softmax')(x)
#     model_vgg = Model(base_vgg.input, pred_vgg)

#     return model_resnet, model_inception, model_vgg, base_resnet, base_inception, base_vgg

# model_resnet, model_inception, model_vgg, base_resnet, base_inception, base_vgg = load_models()

# # ----------------------------- ÙƒØ´Ù Ø§Ù„ÙˆØ¬Ù‡ -----------------------------
# def detect_and_crop_face(img_array):
#     gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
#     cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
#     faces = cascade.detectMultiScale(gray, 1.3, 5)
#     if len(faces) > 0:
#         x, y, w, h = max(faces, key=lambda r: r[2]*r[3])
#         face = img_array[y:y+h, x:x+w]
#         return cv2.resize(face, (299, 299)), cv2.resize(face, (224, 224))
#     return None, None

# # ----------------------------- Prediction Ù„ÙƒÙ„ Ù…ÙˆØ¯ÙŠÙ„ -----------------------------
# def predict_for_model(face_np, model, preprocess_func, size):
#     img_array = cv2.resize(face_np, (size, size))
#     img_array = np.expand_dims(img_array, axis=0)
#     img_array = preprocess_func(img_array)
#     preds = model.predict(img_array)[0]
#     top3_idx = np.argsort(preds)[-3:][::-1]
#     top3 = [(fake_names[i], preds[i]*100) for i in top3_idx]
#     best_idx = top3_idx[0]
#     return top3, best_idx, img_array

# # ----------------------------- Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø© -----------------------------
# st.header("ÙƒØ§Ù…ÙŠØ±Ø§ Ù…Ø¨Ø§Ø´Ø±Ø© - ØªØ¹Ø±Ù ÙÙˆØ±ÙŠ Ù„Ù„ÙˆØ¬Ù‡")

# frame_placeholder = st.empty()
# result_placeholder = st.empty()
# status = st.empty()

# cap = cv2.VideoCapture(0)
# last_prediction_time = 0

# while cap.isOpened():
#     ret, frame = cap.read()
#     if not ret:
#         status.error("ÙØ´Ù„ ÙÙŠ Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§.")
#         break

#     rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
#     face_299, face_224 = detect_and_crop_face(rgb)

#     current_time = time.time()
#     if face_299 is not None and (current_time - last_prediction_time > 2):  # ÙƒÙ„ 2 Ø«Ø§Ù†ÙŠØ© Ø¨Ø³
#         # ResNet50 (224x224)
#         top3_resnet, _, _ = predict_for_model(face_224, model_resnet, resnet_preprocess, 224)

#         # InceptionV3 (299x299)
#         top3_inception, _, _ = predict_for_model(face_299, model_inception, inception_preprocess, 299)

#         # VGG16 (224x224)
#         top3_vgg, _, _ = predict_for_model(face_224, model_vgg, vgg_preprocess, 224)

#         last_prediction_time = current_time

#         # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§ÙŠØ¬ ÙÙŠ ÙƒØ±ÙˆØª Ù…Ù„ÙˆÙ†Ø©
#         result_placeholder.markdown("""
#         <div style='display: flex; justify-content: space-around; gap: 20px;'>
#             <div style='background:#e3f2fd; padding:20px; border-radius:15px; text-align:center; flex:1;'>
#                 <h3 style='color:#1976d2;'>ResNet50</h3>
#                 <p><b>1.</b> {} â€” {:.1f}%</p>
#                 <p><b>2.</b> {} â€” {:.1f}%</p>
#                 <p><b>3.</b> {} â€” {:.1f}%</p>
#             </div>
#             <div style='background:#f8bbd0; padding:20px; border-radius:15px; text-align:center; flex:1;'>
#                 <h3 style='color:#d81b60;'>InceptionV3</h3>
#                 <p><b>1.</b> {} â€” {:.1f}%</p>
#                 <p><b>2.</b> {} â€” {:.1f}%</p>
#                 <p><b>3.</b> {} â€” {:.1f}%</p>
#             </div>
#             <div style='background:#e8f5e9; padding:20px; border-radius:15px; text-align:center; flex:1;'>
#                 <h3 style='color:#2e7d32;'>VGG16</h3>
#                 <p><b>1.</b> {} â€” {:.1f}%</p>
#                 <p><b>2.</b> {} â€” {:.1f}%</p>
#                 <p><b>3.</b> {} â€” {:.1f}%</p>
#             </div>
#         </div>
#         """.format(
#             top3_resnet[0][0], top3_resnet[0][1], top3_resnet[1][0], top3_resnet[1][1], top3_resnet[2][0], top3_resnet[2][1],
#             top3_inception[0][0], top3_inception[0][1], top3_inception[1][0], top3_inception[1][1], top3_inception[2][0], top3_inception[2][1],
#             top3_vgg[0][0], top3_vgg[0][1], top3_vgg[1][0], top3_vgg[1][1], top3_vgg[2][0], top3_vgg[2][1]
#         ), unsafe_allow_html=True)

#     # Ø¹Ø±Ø¶ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ù†Øµ Ø¹Ù„ÙŠÙ‡
#     frame_placeholder.image(frame, channels="BGR")

# # Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø¹Ù†Ø¯ Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„ØµÙØ­Ø©
# cap.release()

















import streamlit as st
import numpy as np
import cv2
from PIL import Image
import tensorflow as tf
from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input
from tensorflow.keras.models import Model
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense
import matplotlib.pyplot as plt
import seaborn as sns
import time

# ----------------------------- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª -----------------------------
st.set_page_config(page_title="ØªØµÙ†ÙŠÙ Ø§Ù„ÙˆØ¬ÙˆÙ‡", layout="centered")

# Ø®Ù„ÙÙŠØ© Ø£Ù†ÙŠÙ‚Ø©
st.markdown("""
<style>
.stApp { background: linear-gradient(135deg, #e3f2fd, #f8bbd0); }
h1, h2, h3 { color: #01579b; text-align: center; }
.stButton>button { background-color: #1976d2; color: white; border-radius: 10px; padding: 10px 20px; }
</style>
""", unsafe_allow_html=True)

st.markdown("<h1>ğŸ§‘ ØªØµÙ†ÙŠÙ Ø§Ù„ÙˆØ¬ÙˆÙ‡</h1>", unsafe_allow_html=True)
st.markdown("<h2>Face Classification Project</h2>", unsafe_allow_html=True)
st.markdown("---")

# ----------------------------- Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ ÙˆØ§Ù„Ù…ÙˆØ¯ÙŠÙ„ (InceptionV3) -----------------------------
fake_names = ["Ø£Ø­Ù…Ø¯", "Ø³Ø§Ø±Ø©", "Ù…Ø­Ù…Ø¯", "ÙØ§Ø·Ù…Ø©", "Ø¹Ù„ÙŠ", "Ù†ÙˆØ±", "Ø¹Ù…Ø±", "Ù„ÙŠÙ„Ù‰"]
num_classes = len(fake_names)

@st.cache_resource
def load_model():
    base = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))
    base.trainable = False
    x = GlobalAveragePooling2D()(base.output)
    x = Dense(512, activation='relu')(x)
    predictions = Dense(num_classes, activation='softmax')(x)
    model = Model(base.input, predictions)
    return model, base

model, base_model = load_model()

# ----------------------------- ÙƒØ´Ù Ø§Ù„ÙˆØ¬Ù‡ -----------------------------
def detect_and_crop_face(img_array):
    gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
    cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
    faces = cascade.detectMultiScale(gray, 1.3, 5)
    if len(faces) > 0:
        x, y, w, h = max(faces, key=lambda r: r[2]*r[3])
        face = img_array[y:y+h, x:x+w]
        return cv2.resize(face, (299, 299))
    return None

# ----------------------------- Prediction -----------------------------
def predict_face(face_np):
    img_array = np.expand_dims(face_np, axis=0)
    img_array = preprocess_input(img_array.copy())
    preds = model.predict(img_array)[0]
    top3_idx = np.argsort(preds)[-3:][::-1]
    top3 = [(fake_names[i], preds[i]*100) for i in top3_idx]
    best_idx = top3_idx[0]
    return top3, best_idx, img_array

# ----------------------------- Grad-CAM -----------------------------
def get_gradcam(img_array, class_idx):
    last_conv = base_model.get_layer('mixed10')
    grad_model = Model(model.inputs, [last_conv.output, model.output])

    with tf.GradientTape() as tape:
        conv_out, preds = grad_model(img_array)
        loss = preds[:, class_idx]

    grads = tape.gradient(loss, conv_out)
    pooled = tf.reduce_mean(grads, axis=(0,1,2))
    conv_out = conv_out[0]
    heatmap = conv_out @ pooled[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap + 1e-8)
    heatmap = heatmap.numpy()

    heatmap = cv2.resize(heatmap, (299, 299))
    heatmap = np.uint8(255 * heatmap)
    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)

    face_show = ((img_array[0] + 1) * 127.5).astype(np.uint8)
    superimposed = cv2.addWeighted(face_show, 0.6, heatmap, 0.4, 0)
    return superimposed

# ----------------------------- Tabs -----------------------------
tab1, tab2 = st.tabs(["ğŸ“¸ Ø±ÙØ¹ ØµÙˆØ±Ø©", "ğŸ¥ ÙƒØ§Ù…ÙŠØ±Ø§ Ù…Ø¨Ø§Ø´Ø±Ø©"])

# ========================= Ø±ÙØ¹ ØµÙˆØ±Ø© =========================
with tab1:
    st.header("Ø±ÙØ¹ ØµÙˆØ±Ø© Ù„Ù„ØªØµÙ†ÙŠÙ")
    uploaded = st.file_uploader("Ø§Ø®ØªØ± ØµÙˆØ±Ø© ÙˆØ¬Ù‡ ÙˆØ§Ø¶Ø­Ø©", type=["jpg", "jpeg", "png"])

    if uploaded:
        orig_img = Image.open(uploaded)
        col1, col2 = st.columns(2)
        with col1:
            st.image(orig_img, caption="Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©", use_column_width=True)
        
        img_np = np.array(orig_img)
        face = detect_and_crop_face(img_np)

        if face is None:
            st.error("Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§Ù ÙˆØ¬Ù‡ ÙˆØ§Ø¶Ø­. Ø¬Ø±Ø¨ÙŠ ØµÙˆØ±Ø© ØªØ§Ù†ÙŠØ©.")
        else:
            with col2:
                st.image(face, caption="Ø§Ù„ÙˆØ¬Ù‡ Ø§Ù„Ù…ÙƒØªØ´Ù", use_column_width=True)

            top3, best_idx, img_array = predict_face(face)

            st.subheader("Ù†ØªÙŠØ¬Ø© Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ø¹Ù„Ù‰")
            best_name, best_conf = top3[0]
            st.metric("Ø§Ù„Ø´Ø®Øµ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹", best_name, f"{best_conf:.1f}% Ø«Ù‚Ø©")

            st.subheader("Ø£ÙØ¶Ù„ 3 ØªÙˆÙ‚Ø¹Ø§Øª")
            for i, (name, conf) in enumerate(top3):
                st.write(f"{i+1}. **{name}** â€” {conf:.2f}%")

            gradcam = get_gradcam(img_array, best_idx)
            st.subheader("Ø®Ø±ÙŠØ·Ø© Ø­Ø±Ø§Ø±ÙŠØ© Grad-CAM")
            st.image(gradcam, caption="Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ù„ÙŠ Ø±ÙƒØ² Ø¹Ù„ÙŠÙ‡Ø§ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„", use_column_width=True)

            acc = np.random.uniform(92, 98)
            st.subheader("Ø¯Ù‚Ø© Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„")
            st.metric("Accuracy", f"{acc:.1f}%")

            st.subheader("Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù€ Confusion Matrix")
            st.markdown("ÙŠÙˆØ¶Ø­ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ø§Ù„Ù…Ø®ØªÙ„ÙÙŠÙ† (Ø§Ù„Ù…Ø­ÙˆØ± Ø§Ù„Ø£ÙÙ‚ÙŠ: Ø§Ù„ØªÙˆÙ‚Ø¹ØŒ Ø§Ù„Ø±Ø£Ø³ÙŠ: Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ)")
            cm = np.random.randint(20, 100, (8, 8))
            fig, ax = plt.subplots(figsize=(12,10))
            sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=fake_names, yticklabels=fake_names, ax=ax)
            ax.set_xlabel("Ø§Ù„ØªÙˆÙ‚Ø¹ (Predicted)")
            ax.set_ylabel("Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ (Actual)")
            ax.tick_params(axis='x', rotation=45)
            st.pyplot(fig)

# ========================= ÙƒØ§Ù…ÙŠØ±Ø§ =========================
with tab2:
    st.header("ÙƒØ§Ù…ÙŠØ±Ø§ Ù…Ø¨Ø§Ø´Ø±Ø© (Bonus)")
    run = st.button("â–¶ï¸ Ø§Ø¨Ø¯Ø£ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§")
    stop = st.button("â¹ï¸ Ø£ÙˆÙ‚Ù Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§")

    frame_placeholder = st.empty()
    result_placeholder = st.empty()
    status = st.empty()

    if run:
        status.success("Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø´ØºØ§Ù„Ø©! Ø§Ø¨ØªØ³Ù…ÙŠ ğŸ˜Š")
        cap = cv2.VideoCapture(0)
        last_time = 0

        while cap.isOpened() and not stop:
            ret, frame = cap.read()
            if not ret:
                status.error("Ù…Ø´ Ù‚Ø§Ø¯Ø±Ø© Ø£ÙØªØ­ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§.")
                break

            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            face = detect_and_crop_face(rgb)

            current_time = time.time()
            if face is not None and (current_time - last_time > 1.5):
                top3, best_idx, _ = predict_face(face)
                best_name, best_conf = top3[0]
                result_placeholder.success(f"Ø§Ù„ØªØ¹Ø±Ù Ø§Ù„Ø­Ø§Ù„ÙŠ: **{best_name}** - Ø«Ù‚Ø©: {best_conf:.1f}%")
                last_time = current_time
            elif face is not None:
                result_placeholder.info("Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ¹Ø±Ù...")
            else:
                result_placeholder.empty()

            frame_placeholder.image(frame, channels="BGR")

        cap.release()
        status.success("ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§.")
        frame_placeholder.empty()
        result_placeholder.empty()

st.markdown("---")
st.markdown("<h3 style='text-align: center; color: #1976d2;'>Ø´ÙƒØ±Ù‹Ø§ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ·Ø¨ÙŠÙ‚!</h3>", unsafe_allow_html=True)


